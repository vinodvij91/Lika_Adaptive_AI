# Lika Sciences Platform

## Overview
Lika Sciences is a scientific SaaS platform for drug discovery and molecular research, focusing on managing molecule registries, research campaigns, and SMILES data. It aims to streamline drug discovery workflows, accelerate molecular research, and enable data-driven decisions in scientific R&D through clear information, robust data visualization, and professional scientific presentation, with future extensibility for advanced chemistry computations.

## User Preferences
Preferred communication style: Simple, everyday language.

## System Architecture
The platform utilizes a full-stack TypeScript architecture with React, Vite, shadcn/ui, and Tailwind CSS for the frontend, using TanStack React Query for state management. The backend is built with Node.js, Express, TypeScript, and PostgreSQL, employing Drizzle ORM and drizzle-zod for database interactions and schema validation. Shared schemas and types are managed in a `shared/` directory.

### Key Domain Features
- **Molecule & Project Management**: Manages chemical compounds (SMILES notation) and organizes them into projects and research campaigns with various modalities (small molecule, PROTAC, peptide, fragment).
- **Data Import & Curation**: Supports batch import of SMILES with validation, duplicate detection, and provides curated domain-aware SMILES libraries.
- **Scientific Terminology**: Integrates professional drug discovery terminology across the platform.
- **Compute Nodes**: Manages multi-provider infrastructure (Hetzner, Vast.ai, AWS, Azure, GCP, On-Prem) for ML, docking, quantum, and agent workloads, including SSH key management and usage tracking.
- **Advanced Scientific Modules**: Includes Multi-Target SAR for profiling and visualization, Translational Medicine for target variants and extended molecule scoring, Wet-Lab Integration for assay definition and experiment recommendations, and Literature & IP for annotations and risk screening.
- **Collaboration & Templates**: Supports multi-organization collaboration with role-based access and provides disease-specific pipeline templates.
- **Materials Science**: A first-class domain supporting polymers, crystals, composites, catalysts, membranes, and coatings with both structure-first and property-first discovery modes.

### Materials Science Data Model
Core tables include `material_entities` for material identity and representation, `material_variants` for structural exploration, `material_properties` for predictions, `materials_programs` for high-level research, `materials_campaigns` for discovery, and `materials_oracle_scores` for scoring.

### Internal Normalized Schemas
The Import Hub converts diverse industry formats into canonical tables, performing deduplication and normalization for both drug discovery (e.g., `canonical_molecules`, `molecule_descriptors`, `canonical_assays`) and materials science (e.g., `canonical_materials`, `canonical_material_properties`).

### Asset Storage Strategy
DigitalOcean Spaces (S3-compatible) is used for primary storage of assets generated by compute nodes, with metadata stored in a `compound_assets` PostgreSQL table. Assets are served via CDN.

### Enterprise-Scale Processing Infrastructure
Designed for industrial-scale materials discovery, the platform features a robust processing jobs system (`processing_jobs` table) for orchestration, status tracking, and fault tolerance. It includes `processing_job_runs` for retry management, `processing_job_events` for streaming results, and `job_artifacts` for storing computation outputs, with an artifact ingestion system to process Python computation results. Precomputed aggregations (`materials_campaign_aggregates`, `material_variant_metrics`) provide dashboard data and per-variant metrics.

### API Design
The platform uses RESTful API endpoints under `/api/`, with specific `/api/agent/` endpoints tailored for AI interaction and service account roles defining agent permissions.

### Design System
Adheres to Material Design principles with Carbon Design data patterns, utilizing Inter and JetBrains Mono fonts, and a custom Tailwind theme for scientific visualization and data-dense layouts.

### Quantum Compute Integration
The architecture is designed to integrate with quantum computation services for future optimization and scoring.

### Python Compute Pipeline Integration
The platform includes a production-grade Python pipeline (`compute/drug_discovery_pipeline.py`) for executing computational chemistry workloads:

**Features:**
- Dask distributed computing for parallel processing
- RAPIDS cuML for GPU-accelerated machine learning (10-50x speedup)
- PyTorch mixed precision training (2x GPU speedup)
- AutoDock Vina integration for molecular docking
- XGBoost with GPU acceleration

**Compute Infrastructure:**
- **CPU Jobs (Hetzner)**: SMILES validation, fingerprint generation, property calculation, rule filtering
- **GPU Jobs (Vast.ai with 2x RTX 3090)**: ML prediction, neural network inference, Vina docking

**CLI Interface:**
```bash
python3 drug_discovery_pipeline.py --job-type <step_name> --params '{"smiles": [...], "targetId": "..."}'
```

**Supported Steps:**
- `smiles_validation` - Validate SMILES strings
- `fingerprint_generation` - Generate Morgan fingerprints
- `property_calculation` - Calculate molecular properties (MW, LogP, TPSA, etc.)
- `ml_prediction` - Run ML predictions on molecules
- `vina_docking` - Molecular docking with AutoDock Vina
- `scoring` - Calculate oracle scores
- `rule_filtering` - Apply Lipinski/Veber rules

**Required Environment Variables:**
- `HETZNER_SSH_HOST`, `HETZNER_SSH_USER`, `HETZNER_SSH_PORT` - For Hetzner CPU nodes
- `VAST_AI_API_KEY` - For Vast.ai GPU nodes
- `SSH_PRIVATE_KEY` - SSH key for remote node access

**API Endpoints:**
- `GET /api/compute/health` - Check node health
- `GET /api/compute/capacity` - Get available CPU/GPU capacity
- `POST /api/compute/execute` - Execute a pipeline job (async by default)
- `POST /api/compute/pipeline` - Run full pipeline on campaign
- `POST /api/compute/setup` - Auto-configure nodes from env vars

### Materials Science Compute Pipeline
The platform includes a comprehensive Materials Science pipeline (`compute/materials_science_pipeline.py`) for materials discovery:

**Features:**
- **Magpie Descriptors**: 145 compositional descriptors based on elemental properties
- **SOAP Descriptors**: Smooth Overlap of Atomic Positions for structural fingerprinting (via DScribe)
- **Graph Neural Networks**: Crystal Graph Convolutional Neural Network (CGCNN) and Multi-Property GNN for structure-aware property prediction
- **Multi-task Neural Networks**: Shared encoder with property-specific heads
- **GPU Acceleration**: Mixed precision training, batch prediction with PyTorch
- **Materials Generation**: Compositional sampling with target property optimization
- **Element Substitution**: Systematic variant generation using substitution rules
- **Synthesis Planning**: Route suggestions, precursor identification, synthesizability scoring
- **Atomistic Simulations**: Structure optimization, MD, and band structure via ASE

**Specialized Designers:**
- `BatteryMaterialsDesigner` - Cathode discovery (voltage, capacity targets)
- `PhotovoltaicMaterialsDesigner` - Solar absorber discovery (band gap optimization)
- `StructuralMaterialsDesigner` - High-strength alloy discovery (modulus, density)

**Specialized Discovery Workflows:**
- `SuperconductorDiscovery` - High-Tc superconductor discovery with DFT validation (cuprates, iron-based, hydrides)
- `CatalystDiscovery` - HER and ORR catalyst discovery for fuel cells and electrolyzers
- `ThermoelectricDiscovery` - High-ZT thermoelectric materials discovery

**DFT Calculators:**
- `VASPCalculator` - VASP integration for DFT calculations (relax, static, band, dos)
- `QuantumESPRESSOCalculator` - Quantum ESPRESSO integration for open-source DFT

**Materials Project API (Official mp-api Package):**
The pipeline now uses the official `mp-api` package with `MPRester` client for robust API access:
```python
from mp_api.client import MPRester
mpr = MPRester(api_key)
docs = mpr.summary.search(elements=['Li', 'Fe'], band_gap=(1.0, 2.0))
battery_data = mpr.insertion_electrodes.search(working_ion='Li')
phase_diagram = mpr.get_entries_in_chemsys(['Li', 'Fe', 'O'])
```

**CLI Interface:**
```bash
python3 materials_science_pipeline.py --job-type <step_name> --params '{"materials": [...], "target_properties": {...}}'
```

**Supported Steps:**
- `structure_validation` - Validate material representations (CIF, formula, SMILES)
- `magpie_descriptors` - Generate Magpie compositional descriptors
- `soap_descriptors` - Generate SOAP structural descriptors
- `gnn_prediction` - GNN-based property prediction for crystals
- `property_prediction` - Multi-task neural network property prediction
- `manufacturability_scoring` - Score synthesis feasibility
- `synthesis_planning` - Generate synthesis routes
- `batch_screening` - High-throughput property screening
- `materials_generation` - Generate novel materials with target properties
- `element_substitution` - Generate variants by element substitution
- `atomistic_simulation` - Run MD/DFT simulations
- `full_pipeline` - Complete discovery workflow

**Materials Science API Endpoints:**
- `POST /api/compute/materials/predict` - Property prediction
- `POST /api/compute/materials/manufacturability` - Manufacturability scoring
- `POST /api/compute/materials/screen` - Batch screening
- `POST /api/compute/materials/validate` - Structure validation
- `POST /api/compute/materials/magpie` - Magpie descriptors
- `POST /api/compute/materials/soap` - SOAP descriptors
- `POST /api/compute/materials/gnn` - GNN prediction
- `POST /api/compute/materials/synthesis` - Synthesis planning
- `POST /api/compute/materials/generate` - Materials generation
- `POST /api/compute/materials/substitute` - Element substitution
- `POST /api/compute/materials/simulate` - Atomistic simulation
- `POST /api/compute/materials/discover` - Full discovery pipeline

**Materials Project API Integration Endpoints:**
- `POST /api/compute/materials/mp/training-data` - Load training data for any property
- `POST /api/compute/materials/mp/battery` - Load battery electrode data (Li, Na, K ion)
- `POST /api/compute/materials/mp/solar` - Load solar absorber candidates
- `POST /api/compute/materials/mp/thermoelectric` - Load thermoelectric materials
- `POST /api/compute/materials/mp/superconductor` - Load superconductor candidates
- `POST /api/compute/materials/mp/phase-diagram` - Generate phase diagrams
- `POST /api/compute/materials/mp/bulk-query` - Bulk property queries
- `POST /api/compute/materials/mp/search` - Search by formula

**Materials Project CLI Steps:**
- `mp_load_training_data` - Load training data for ML models
- `mp_load_battery_data` - Load battery electrode materials
- `mp_load_solar_materials` - Load solar absorber candidates
- `mp_load_thermoelectric_materials` - Load thermoelectric materials
- `mp_load_superconductor_candidates` - Load superconductor candidates
- `mp_get_phase_diagram` - Generate phase diagrams
- `mp_bulk_query` - Bulk property queries
- `mp_search_formula` - Search by formula

**Required Environment Variables:**
- `MP_API_KEY` - Materials Project API key (get from https://materialsproject.org/api)
- `VASP_PP_PATH` - Directory containing VASP POTCAR files (for DFT calculations)
- `ESPRESSO_PSEUDO` - Directory containing Quantum ESPRESSO pseudopotentials

**Required Python Packages (on compute nodes):**
- `pymatgen` - Crystal structure analysis
- `mp-api` - Official Materials Project API client
- `torch` + `torch-geometric` - GNN and neural networks
- `dscribe` - SOAP descriptors
- `ase` - Atomistic simulations (with VASP and Espresso calculators)
- `dask` + `distributed` - Parallel computing
- `qiskit` (optional) - Quantum simulations

## External Dependencies
- **PostgreSQL**: Primary relational database.
- **Radix UI primitives**: For building UI components.
- **cmdk**: For command palette functionality.
- **embla-carousel-react**: For carousel components.
- **date-fns**: For date utility functions.